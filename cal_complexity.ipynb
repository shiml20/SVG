{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_complexity_attn(E, k, H, D, d, N):\n",
    "    # 计算各个部分\n",
    "    part1 = 3 * N * D * H * d  # O(3NDHd)\n",
    "    part2 = H * N**2 * d        # O(HN^2d)\n",
    "    part3 = H * N**2 * d        # O(HN^2d)\n",
    "    part4 = N * H * d * D       # O(NHdD)\n",
    "\n",
    "    # 总复杂度\n",
    "    total_complexity = part1 + part2 + part3 + part4\n",
    "    return total_complexity\n",
    "\n",
    "def calculate_complexity_mlp(E, k, H, D, d, N):\n",
    "    # 计算各个部分\n",
    "    part1 = 8 * N * D * D  # O(8ND^2)\n",
    "\n",
    "    # 总复杂度\n",
    "    total_complexity = part1\n",
    "    return total_complexity\n",
    "\n",
    "def calculate_complexity_moha(E, k, H, D, d, N):\n",
    "    # 计算各个部分\n",
    "    part1 = 3 * k * D * H * d  # O(3NDHd)\n",
    "    part2 = H * k**2 * d        # O(HN^2d)\n",
    "    part3 = H * k**2 * d        # O(HN^2d)\n",
    "    part4 = k * H * d * D       # O(NHdD)\n",
    "\n",
    "    # 总复杂度\n",
    "    total_complexity = part1 + part2 + part3 + part4\n",
    "    return total_complexity * E\n",
    "\n",
    "# 输入值\n",
    "capacity = 2\n",
    "config = [\n",
    "    {\n",
    "        'N':256,\n",
    "        'H':6, 'D':384,'d':384 // 6, \n",
    "        'k':16 * capacity, \n",
    "        'E':16,\n",
    "    },\n",
    "    {\n",
    "        'N':256, \n",
    "        'H':12, 'D':768, 'd':768 // 12,\n",
    "        'k':16 * capacity, \n",
    "        'E':16,\n",
    "\n",
    "    },\n",
    "    {\n",
    "        'N':256,\n",
    "        'H':16, 'D':1024, 'd':1024 // 16,\n",
    "        'k':16 * capacity, \n",
    "        'E':16,\n",
    "\n",
    "    },\n",
    "    {\n",
    "        'N':256,\n",
    "        'H':16, 'D':1152, 'd':1152 // 16,\n",
    "        'k':16 * capacity, \n",
    "        'E':16,\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "# 计算复杂度\n",
    "for i in range(4):\n",
    "    result_attn = calculate_complexity_attn(**config[i])\n",
    "    result_mlp = calculate_complexity_mlp(**config[i])\n",
    "    result_moba = calculate_complexity_moha(**config[i])\n",
    "    print(f\"Mlp复杂度为:  {result_mlp/1000_000}\")\n",
    "    print(f\"Attn复杂度为: {result_attn/1000_000} MoHA复杂度为:  {result_moba/1000_000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = [\n",
    "    {\n",
    "        'N':1024,\n",
    "        'H':6, 'D':384,'d':384 // 6, \n",
    "        'k':64 * capacity, \n",
    "        'E':16,\n",
    "    },\n",
    "    {\n",
    "        'N':1024, \n",
    "        'H':12, 'D':768, 'd':768 // 12,\n",
    "        'k':64 * capacity, \n",
    "        'E':16,\n",
    "\n",
    "    },\n",
    "    {\n",
    "        'N':1024,\n",
    "        'H':16, 'D':1024, 'd':1024 // 16,\n",
    "        'k':64 * capacity, \n",
    "        'E':16,\n",
    "\n",
    "    },\n",
    "    {\n",
    "        'N':1024,\n",
    "        'H':16, 'D':1152, 'd':1152 // 16,\n",
    "        'k':64 * capacity, \n",
    "        'E':16,\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "# 计算复杂度\n",
    "for i in range(4):\n",
    "    result_attn = calculate_complexity_attn(**config[i])\n",
    "    result_mlp = calculate_complexity_mlp(**config[i])\n",
    "    result_moba = calculate_complexity_moha(**config[i])\n",
    "    print(f\"Mlp复杂度为:  {result_mlp/1000_000}\")\n",
    "    print(f\"Attn复杂度为: {result_attn/1000_000} MoHA复杂度为:  {result_moba/1000_000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_complexity_3dattn_tokenmerge(H, D, d, N):\n",
    "    # 计算各个部分\n",
    "    N = N // 4\n",
    "    part1 = 3 * N * D * H * d  # O(3NDHd)\n",
    "    part2 = H * N**2 * d        # O(HN^2d)\n",
    "    part3 = H * N**2 * d        # O(HN^2d)\n",
    "    part4 = N * H * d * D       # O(NHdD)\n",
    "    # 总复杂度\n",
    "    total_complexity = part1 + part2 + part3 + part4\n",
    "    return total_complexity\n",
    "\n",
    "def calculate_complexity_2dattn(H, D, d, N):\n",
    "    # 计算各个部分\n",
    "    T = 20\n",
    "    N = N / T\n",
    "    part1 = 3 * N * D * H * d  # O(3NDHd)\n",
    "    part2 = H * N**2 * d        # O(HN^2d)\n",
    "    part3 = H * N**2 * d        # O(HN^2d)\n",
    "    part4 = N * H * d * D       # O(NHdD)\n",
    "    # 总复杂度\n",
    "    total_complexity = part1 + part2 + part3 + part4\n",
    "    return total_complexity * T\n",
    "\n",
    "def calculate_complexity_swiglu(H, D, d, N):\n",
    "    # 维度映射关系（默认mult=4, scale=1）\n",
    "    inner_dim = 4 * D  # linear1和linear2的输出维度\n",
    "    # 三线性层计算量分解\n",
    "    part1 = N * D * inner_dim   # linear1投影 (N×D→N×4D)\n",
    "    part2 = N * D * inner_dim   # linear2投影 (N×D→N×4D)\n",
    "    part3 = N * inner_dim * D   # linear3投影 (N×4D→N×D)\n",
    "    # 总复杂度（保持与标准FFN对比基准）\n",
    "    total_complexity = part1 + part2 + part3  # O(12ND²) 复杂度\n",
    "    return total_complexity\n",
    "\n",
    "capacity_here = 1\n",
    "config = [\n",
    "    {\n",
    "        'N':1920 * 1080 / 8/2 / 8/2 * 20,\n",
    "        'H':16, 'D':1152, 'd':1152 // 16,\n",
    "    },\n",
    "]\n",
    "# 计算复杂度\n",
    "for i in range(1):\n",
    "    result_3dattn = calculate_complexity_3dattn_tokenmerge(**config[i])\n",
    "    result_2dattn = calculate_complexity_2dattn(**config[i])\n",
    "    result_mlp = calculate_complexity_swiglu(**config[i])\n",
    "    print(f\"swiglu复杂度为:  {result_mlp/1000_000}\")\n",
    "    print(f\"3dAttn复杂度为: {result_3dattn/1000_000} 2dAttn复杂度为: {result_2dattn/1000_000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1920 * 1080 / 8/2 / 8/2 * 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1920 * 1080 / 8/2 / 8/2 * 20 / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1920 * 1080 / 8/2 / 8/2 * 20 / 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "# 假设隐藏状态的形状是 [1000, 120, 4096]\n",
    "hidden_states = torch.load(\"/ytech_m2v2_hdd/sml/DiffMoE_research_local/text_feature.pt\")\n",
    "hidden_states[0, :100, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 设定聚类类别数\n",
    "n_clusters = 10\n",
    "\n",
    "# 用于存储每个 prompt 的聚类标签分布\n",
    "prompt_cluster_distributions = []\n",
    "\n",
    "# 对每个 prompt 的 token 特征进行聚类\n",
    "for i in range(hidden_states.shape[0]):\n",
    "    # 提取当前 prompt 的 token 特征，形状为 [120, 4096]\n",
    "    tokens = hidden_states[i].cpu().numpy()\n",
    "    \n",
    "    # 使用 K-Means 聚类\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(tokens)\n",
    "    \n",
    "    # 计算聚类标签的分布（直方图）\n",
    "    hist, _ = np.histogram(cluster_labels, bins=n_clusters, range=(0, n_clusters))\n",
    "    hist = hist / hist.sum()  # 归一化\n",
    "    prompt_cluster_distributions.append(hist)\n",
    "\n",
    "# 将聚类分布转换为 numpy 数组，形状为 [1000, n_clusters]\n",
    "prompt_cluster_distributions = np.array(prompt_cluster_distributions)\n",
    "\n",
    "# 计算 prompt 之间的相似度矩阵\n",
    "similarity_matrix = np.zeros((1000, 1000))\n",
    "\n",
    "for i in range(1000):\n",
    "    for j in range(1000):\n",
    "        # 使用 Jensen-Shannon 距离计算分布之间的相似度\n",
    "        js_distance = jensenshannon(prompt_cluster_distributions[i], prompt_cluster_distributions[j])\n",
    "        similarity_matrix[i][j] = 1 - js_distance  # 将距离转换为相似度\n",
    "\n",
    "# 打印相似度矩阵\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 设置画布大小\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# 绘制热力图\n",
    "sns.heatmap(\n",
    "    similarity_matrix,\n",
    "    cmap=\"YlOrRd\",  # 颜色映射\n",
    "    vmin=0,         # 最小值\n",
    "    vmax=1,         # 最大值\n",
    "    square=True,    # 使每个单元格为正方形\n",
    "    xticklabels=50, # 每 50 个标签显示一个\n",
    "    yticklabels=50  # 每 50 个标签显示一个\n",
    ")\n",
    "\n",
    "# 添加标题和标签\n",
    "plt.title(\"Prompt Similarity Heatmap (Clustering-Based)\", fontsize=16)\n",
    "plt.xlabel(\"Prompt Index\", fontsize=12)\n",
    "plt.ylabel(\"Prompt Index\", fontsize=12)\n",
    "\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timeit\n",
    "\n",
    "# 原始函数\n",
    "def find_value_indices_original(tensor, max_val):\n",
    "    result = []\n",
    "    for val in range(max_val):\n",
    "        mask = (tensor == val)\n",
    "        indices = torch.nonzero(mask, as_tuple=True)[0]\n",
    "        result.append(indices)\n",
    "    return result\n",
    "\n",
    "# 优化后的函数\n",
    "def find_value_indices_optimized(tensor, max_val):\n",
    "    mask = tensor.unsqueeze(-1) == torch.arange(max_val, device=tensor.device)\n",
    "    return [torch.where(mask[:, i])[0] for i in range(max_val)]\n",
    "\n",
    "# 测试数据\n",
    "def generate_test_data(size, max_val):\n",
    "    return torch.randint(0, max_val, (size,)), max_val\n",
    "\n",
    "# 测试函数性能\n",
    "def test_performance():\n",
    "    sizes = [1000, 10000, 100000]  # 测试不同大小的张量\n",
    "    max_val = 10  # 假设最大值为 10\n",
    "    repeat = 100  # 每个函数运行 100 次取平均\n",
    "\n",
    "    for size in sizes:\n",
    "        tensor, max_val = generate_test_data(size, max_val)\n",
    "        \n",
    "        # 测试原始函数\n",
    "        time_original = timeit.timeit(\n",
    "            lambda: find_value_indices_original(tensor, max_val),\n",
    "            number=repeat\n",
    "        ) / repeat\n",
    "\n",
    "        # 测试优化后的函数\n",
    "        time_optimized = timeit.timeit(\n",
    "            lambda: find_value_indices_optimized(tensor, max_val),\n",
    "            number=repeat\n",
    "        ) / repeat\n",
    "\n",
    "        print(f\"Tensor size: {size}\")\n",
    "        print(f\"Original function: {time_original:.6f} seconds per call\")\n",
    "        print(f\"Optimized function: {time_optimized:.6f} seconds per call\")\n",
    "        print(f\"Speedup: {time_original / time_optimized:.2f}x\")\n",
    "        print(\"---\")\n",
    "\n",
    "# 运行性能测试\n",
    "test_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个张量\n",
    "x = torch.randn(1000000, 512).cuda()\n",
    "\n",
    "# 测量 index_select 的执行时间\n",
    "%timeit x.index_select(0, torch.randint(0, 1000000, (1024,)).cuda())\n",
    "\n",
    "# 测量直接索引的执行时间\n",
    "%timeit x[torch.randint(0, 1000000, (1024,)).cuda()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "\n",
    "def method_loop(mask: torch.Tensor) -> List[torch.Tensor]:\n",
    "    \"\"\"基础循环方法\"\"\"\n",
    "    expert_to_indices = []\n",
    "    for expert_idx in range(mask.size(0)):\n",
    "        positions = mask[expert_idx].nonzero(as_tuple=True)[0]\n",
    "        expert_to_indices.append(positions)\n",
    "    return expert_to_indices\n",
    "\n",
    "def method_vectorized(mask: torch.Tensor) -> List[torch.Tensor]:\n",
    "    \"\"\"向量化split方法\"\"\"\n",
    "    all_indices = mask.nonzero()\n",
    "    if all_indices.size(0) == 0:\n",
    "        return [torch.empty(0, dtype=torch.long, device=mask.device) for _ in range(mask.size(0))]\n",
    "    return torch.split(all_indices[:,1], mask.sum(dim=1).tolist())\n",
    "\n",
    "def method_optimized(mask: torch.Tensor) -> List[torch.Tensor]:\n",
    "    \"\"\"unique+split优化方法\"\"\"\n",
    "    expert_ids, positions = mask.nonzero(as_tuple=True)\n",
    "    if expert_ids.numel() == 0:\n",
    "        return [torch.empty(0, dtype=torch.long, device=mask.device) for _ in range(mask.size(0))]\n",
    "    \n",
    "    sorted_ids, sort_idx = expert_ids.sort()\n",
    "    counts = torch.bincount(sorted_ids, minlength=mask.size(0))\n",
    "    return torch.split(positions[sort_idx], counts.tolist())\n",
    "\n",
    "def generate_mask(E: int, L: int, sparsity: float, device='cpu') -> torch.Tensor:\n",
    "    \"\"\"生成指定稀疏度的随机掩码\"\"\"\n",
    "    mask = torch.rand(E, L, device=device) < sparsity\n",
    "    # 确保每行至少有一个非零元素\n",
    "    mask[torch.arange(E, device=device), torch.randint(0, L, (E,), device=device)] = True\n",
    "    return mask\n",
    "\n",
    "def compare_results(ref: List[torch.Tensor], test: List[torch.Tensor]) -> bool:\n",
    "    \"\"\"安全的结果比较方法\"\"\"\n",
    "    if len(ref) != len(test):\n",
    "        return False\n",
    "    for r, t in zip(ref, test):\n",
    "        if r.device != t.device:\n",
    "            t = t.to(r.device)\n",
    "        if not torch.equal(r, t):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def run_benchmark(configs):\n",
    "    # 测试配置\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Running on {device.upper()}\")\n",
    "    \n",
    "\n",
    "    methods = {\n",
    "        'Loop': method_loop,\n",
    "        'Vectorized': method_vectorized,\n",
    "        'Optimized': method_optimized\n",
    "    }\n",
    "    \n",
    "    results = {name: [] for name in methods}\n",
    "    \n",
    "    for config in configs:\n",
    "        print(f\"\\nBenchmarking E={config['E']}, L={config['L']}, sparsity={config['sparsity']}\")\n",
    "        mask = generate_mask(config['E'], config['L'], config['sparsity'], device)\n",
    "        \n",
    "        # 预热和同步\n",
    "        _ = method_optimized(mask)\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        # 获取参考结果\n",
    "        ref_result = method_loop(mask.cpu())\n",
    "        \n",
    "        for name, method in methods.items():\n",
    "            try:\n",
    "                # 计时\n",
    "                def wrapper():\n",
    "                    if device == 'cuda':\n",
    "                        torch.cuda.synchronize()\n",
    "                    return method(mask)\n",
    "                \n",
    "                times = timeit.repeat(wrapper, number=10, repeat=3)\n",
    "                avg_time = sum(times) / len(times) / 10 * 1000  # 平均毫秒时间\n",
    "                \n",
    "                # 验证结果\n",
    "                test_result = method(mask)\n",
    "                if not compare_results(ref_result, test_result):\n",
    "                    print(f\"Validation failed for {name}!\")\n",
    "                    print(\"Reference:\", [r.shape for r in ref_result])\n",
    "                    print(\"Test:\", [t.shape for t in test_result])\n",
    "                    continue\n",
    "                \n",
    "                results[name].append(avg_time)\n",
    "                print(f\"{name:<10}: {avg_time:.4f} ms\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error in {name}: {str(e)}\")\n",
    "                results[name].append(float('nan'))\n",
    "    \n",
    "    # 可视化结果\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    x_labels = [f\"E={c['E']}\\nL={c['L']}\\nsp={c['sparsity']}\" for c in configs]\n",
    "    \n",
    "    for name, times in results.items():\n",
    "        plt.plot(x_labels, times, marker='o', label=name)\n",
    "    \n",
    "    plt.title(f\"Expert Assignment Benchmark ({device.upper()})\")\n",
    "    plt.ylabel(\"Execution Time (ms)\")\n",
    "    plt.xlabel(\"Configuration\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"expert_assignment_benchmark_{device}.png\")\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "\n",
    "    configs = [\n",
    "        {'E': 8, 'L': 1024, 'sparsity': 0.1},\n",
    "        {'E': 16, 'L': 2048, 'sparsity': 0.05},\n",
    "        {'E': 32, 'L': 256*32, 'sparsity': 0.05},\n",
    "        {'E': 32, 'L': 256*32, 'sparsity': 1/32},\n",
    "        {'E': 32, 'L': 256*32, 'sparsity': 0},\n",
    "        {'E': 32, 'L': 256*32, 'sparsity': 1},\n",
    "    ]\n",
    "\n",
    "    run_benchmark(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_vectorized(mask: torch.Tensor) -> List[torch.Tensor]:\n",
    "    \"\"\"向量化split方法\"\"\"\n",
    "    all_indices = mask.nonzero()\n",
    "    if all_indices.size(0) == 0:\n",
    "        return [torch.empty(0, dtype=torch.long, device=mask.device) for _ in range(mask.size(0))]\n",
    "    return torch.split(all_indices[:,1], mask.sum(dim=1).tolist())\n",
    "config = configs[0]\n",
    "mask = generate_mask(config['E'], config['L'], config['sparsity'])\n",
    "len(method_vectorized(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_snr_curve(sigma1=1.0, sigma2=0.3, num_points=200):\n",
    "    alphas = np.linspace(0, 1, num_points)\n",
    "    \n",
    "    # 信号 = (1-a)^2 σ1^2\n",
    "    signal = (1 - alphas)**2 * sigma1**2\n",
    "    # 噪声 = a^2 σ2^2\n",
    "    noise = (alphas**2) * sigma2**2\n",
    "    \n",
    "    snr = signal / (noise + 1e-8)  # 避免除零\n",
    "\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(alphas, snr, label=f\"SNR (σ1={sigma1}, σ2={sigma2})\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"Interpolation α\")\n",
    "    plt.ylabel(\"SNR (log scale)\")\n",
    "    plt.title(\"SNR Curve of Gaussian Interpolation\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, which=\"both\", ls=\"--\")\n",
    "    plt.show()\n",
    "\n",
    "# 默认 σ1=1, σ2=0.3\n",
    "plot_snr_curve(1.0, 1.0)\n",
    "plot_snr_curve(0.3, 1.0)\n",
    "plot_snr_curve(3, 1)\n",
    "\n",
    "# 你可以尝试不同方差：\n",
    "# plot_snr_curve(1.0, 0.5)\n",
    "# plot_snr_curve(1.0, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 总样本数：327979008 * 6 / 256\n",
    "# 全局平均均值：0.017610\n",
    "# 全局平均标准差：0.465989\n",
    "# 总样本数：327979008\n",
    "# 全局平均均值：0.017610\n",
    "# 全局平均标准差：0.386521\n",
    "\n",
    "mean = [-1.310834e-01,  1.005917e-02, -1.615213e-01,  3.709992e-02,  1.624055e-02,\n",
    " -1.072122e-01,  1.153811e-01, -1.764687e-01, -1.564773e-01,  8.010731e-02,\n",
    " -5.455930e-02,  1.026858e-02,  1.395730e-01, -1.677353e-01,  1.416284e-01,\n",
    "  9.624751e-02,  1.084609e-01, -2.143495e-01, -1.287082e-01, -8.222034e-03,\n",
    " -5.636413e-02,  8.554016e-02,  2.261810e-02,  6.544225e-02,  1.489254e-01,\n",
    " -4.932647e-02, -2.373628e-03,  1.595035e-01, -9.460699e-02,  3.337105e-02,\n",
    "  3.061204e-02,  3.003200e-02,  1.734831e-01,  3.377457e-02, -1.021726e-03,\n",
    " -6.289682e-02,  4.971703e-02, -3.472250e-03,  5.610131e-02, -7.359679e-02,\n",
    " -1.849378e-01,  2.845718e-02,  1.893580e-02, -1.241055e-02,  6.904855e-02,\n",
    "  9.557210e-02,  9.049045e-02, -2.354894e-02, -1.821717e-01,  2.173927e-01,\n",
    "  2.623105e-02, -7.295181e-02,  1.777242e-02,  1.112762e+00, -1.263934e-01,\n",
    " -1.298604e-01, -1.510958e-01,  4.459376e-02,  1.828927e-02, -9.131035e-02,\n",
    " -1.995497e-02, -6.222903e-04,  8.596381e-02,  4.136778e-02, -1.578171e-01,\n",
    " -1.337673e-01, -6.054174e-02,  7.156977e-03,  4.524891e-02,  1.216494e-01,\n",
    "  1.205348e-01,  1.033495e-01, -6.540378e-03,  1.981408e-01,  1.220096e-01,\n",
    "  1.036365e-01, -5.540284e-02, -4.624384e-02,  7.136753e-02,  3.484082e-02,\n",
    "  1.984291e-02, -4.273683e-02,  5.246440e-04, -2.307109e-02,  5.547218e-02,\n",
    " -3.933978e-02, -1.458146e-01, -1.129343e-02, -9.579260e-02,  9.284283e-02,\n",
    "  7.371892e-02, -7.637257e-02,  6.592041e-02, -8.843110e-02, -4.001666e-02,\n",
    " -3.148256e-02,  2.121865e-01,  4.097600e-02, -1.293690e-01, -9.589621e-02,\n",
    "  1.258695e-01, -7.101674e-02, -1.331008e-01,  1.493056e-02,  3.541788e+00,\n",
    "  1.420594e-02,  2.701039e-02,  1.175985e-01, -7.834096e-02,  6.380679e-02,\n",
    " -1.653035e-01,  1.550233e-01, -6.797676e-02, -5.019448e-02, -1.963801e-02,\n",
    "  6.645158e-02, -7.009321e-02, -8.109701e-02, -6.408926e-02, -1.683408e-02,\n",
    "  6.049670e-02, -4.398737e-02,  6.312481e-02, -7.364175e-02, -1.522977e-02,\n",
    "  3.527756e-02,  3.163086e-02,  1.245582e-01, -6.687518e-02,  2.580775e-01,\n",
    " -6.847859e-02,  2.051908e-01, -1.123238e-01, -3.409313e-02, -1.933814e-02,\n",
    " -8.682779e-02, -1.050035e-02,  1.283100e-02, -6.638120e-02,  7.655532e-02,\n",
    " -1.195735e-01,  9.798126e-02,  7.605009e-03, -2.112408e-03,  8.539424e-02,\n",
    "  5.429722e-02,  6.980925e-02,  1.459332e-01,  1.205048e-01, -4.054849e-02,\n",
    " -1.245425e-02, -1.228745e-01, -1.627585e-01,  8.031902e-02,  3.292367e-02,\n",
    " -3.371557e-02,  7.386365e-02,  1.811896e-02, -3.936293e-02, -1.017560e-01,\n",
    "  1.424287e-02, -5.966699e-02, -8.968184e-02,  1.662716e-01, -4.915838e-02,\n",
    " -2.737200e-04,  1.414123e-04,  1.011522e-01,  9.977543e-02,  1.224399e-02,\n",
    " -5.626816e-02, -1.061573e-01,  1.235399e-02, -2.361281e-02, -1.172489e-01,\n",
    "  3.690684e-02,  8.034723e-02, -5.032025e-02,  2.349837e-02, -3.136103e-02,\n",
    "  2.231061e-02, -2.128829e-02, -2.515885e-02, -3.194281e-02,  2.816122e-01,\n",
    "  4.188256e-01,  4.135317e-02,  6.339578e-02, -2.925155e-03,  6.940086e-02,\n",
    "  4.784072e-03, -6.444765e-02,  2.542174e-02,  9.924235e-02, -7.811626e-02,\n",
    "  4.030540e-02, -1.033520e-01,  1.386714e-01,  6.809492e-02,  1.093975e-01,\n",
    " -1.256160e-01,  3.689506e-02, -1.856487e-02,  1.850720e-02, -2.637269e-03,\n",
    "  4.571246e-02, -3.289911e-02, -1.077910e-01, -1.388599e-01,  1.328334e-01,\n",
    " -1.743474e-02, -1.308220e-01,  6.733084e-02,  6.298131e-03, -7.544571e-02,\n",
    "  4.740706e-02, -1.270548e-02, -4.204702e-02,  3.411375e-02, -5.634064e-02,\n",
    "  2.137969e-02,  1.725436e-03, -6.205269e-04,  4.907633e-02, -3.407640e-02,\n",
    " -6.360821e-02, -1.577485e-01,  5.634270e-02,  1.308212e-01,  1.170070e+00,\n",
    " -1.998444e-03, -7.140879e-02,  9.566184e-02,  1.889873e-02, -1.816112e+00,\n",
    "  3.908061e-02,  2.467009e-01, -1.227431e-01,  1.497491e-04,  1.018106e-01,\n",
    " -2.515418e-02, -8.447559e-03,  1.499944e-02,  1.022219e-01, -1.997879e-01,\n",
    " -6.360095e-02, -1.213443e-02,  1.518828e-02, -9.520721e-02, -7.753776e-02,\n",
    " -1.922333e-01,  1.406522e-01, -4.912835e-04,  2.104428e-01, -1.150434e-01,\n",
    "  1.147737e-01, -7.678745e-02,  1.235034e-01,  4.065556e-01,  5.981757e-02,\n",
    "  1.165455e-01,  5.487989e-02,  1.984056e-02,  8.687328e-02, -3.526523e-02,\n",
    "  6.872236e-02,  5.857139e-02, -4.834512e-03,  5.304520e-02, -1.590945e-01,\n",
    "  1.254610e-03,  8.046521e-02, -9.876981e-02, -1.112976e-01,  4.069212e-02,\n",
    "  7.972042e-03,  3.892454e-02,  3.154753e-02, -1.163380e-01, -1.605345e-01,\n",
    " -2.693421e-02,  2.297652e-03, -6.363157e-02, -8.444591e-02,  6.560753e-02,\n",
    "  3.419109e-02,  7.927979e-02, -2.809229e-02,  2.600385e-02,  4.144510e-03,\n",
    " -6.732618e-02,  1.544127e-02, -5.075310e-02, -3.481055e-02,  1.736548e-01,\n",
    " -8.164277e-03,  4.955614e-02, -2.437043e-02,  1.747116e-01,  9.048950e-03,\n",
    " -5.758368e-02, -2.327394e-02,  7.905414e-02, -4.244114e-02,  1.366992e-01,\n",
    "  7.400996e-02, -6.351946e-02,  2.505964e-01,  4.201991e-02, -3.152071e-02,\n",
    "  4.283984e-02, -1.246115e-01,  1.215930e-03, -2.306744e-02,  6.073258e-03,\n",
    " -4.608684e-02,  1.002165e-01,  9.539279e-02, -6.975203e-02,  3.450400e-03,\n",
    "  2.194004e-01, -1.432610e-01, -4.289287e-02, -2.807875e-02,  1.326494e-01,\n",
    "  7.334528e-03,  7.678882e-02, -1.392940e-01, -1.010023e-01,  8.594871e-03,\n",
    " -2.403476e-01, -2.865965e-02,  1.044892e-01, -3.868561e-02,  3.841664e-02,\n",
    "  9.703652e-01,  3.661903e-01, -5.823032e-02, -1.782770e-02,  1.497335e-02,\n",
    " -1.635204e-01, -1.064500e-01,  1.317102e-02, -1.928703e-02,  2.493870e-02,\n",
    "  1.109576e-01, -1.168163e-01,  1.212116e-03, -2.840918e-02,  1.174337e-01,\n",
    "  4.394750e-02,  1.290209e-01,  5.055910e-02, -5.226092e-02,  4.705502e-03,\n",
    " -9.981087e-02,  4.069665e-02, -1.311832e-02,  4.786288e-02,  2.846044e-01,\n",
    " -1.231434e-01, -2.413833e-01, -3.822717e-02,  7.206929e-02,  3.475883e-02,\n",
    "  8.923329e-02, -1.087692e-01, -8.315098e-02,  7.632284e-02,  1.032477e-01,\n",
    " -6.948436e-02, -8.174263e-02,  1.075035e-01,  1.116839e-01, -8.545190e-02,\n",
    " -2.812013e-02, -1.043946e-01,  5.158497e-02, -2.432621e-02,  3.674056e-02,\n",
    " -6.595597e-02, -1.876546e-01, -5.529918e-02, -1.807292e-04]\n",
    "\n",
    "std = [0.392849, 0.375289, 0.370822, 0.395598, 0.34522 , 0.405549, 0.361483,\n",
    " 0.366378, 0.31362 , 0.412469, 0.375657, 0.393863, 0.330323, 0.410793,\n",
    " 0.359335, 0.396973, 0.344869, 0.34835 , 0.349905, 0.392577, 0.381021,\n",
    " 0.328242, 0.395749, 0.380014, 0.38424 , 0.365401, 0.364199, 0.401468,\n",
    " 0.428667, 0.374665, 0.342948, 0.434147, 0.331311, 0.349698, 0.369843,\n",
    " 0.374011, 0.514408, 0.362731, 0.406831, 0.357253, 0.322817, 0.339967,\n",
    " 0.353882, 0.359335, 0.408583, 0.376324, 0.368155, 0.344663, 0.406418,\n",
    " 0.37508 , 0.420444, 0.334164, 0.374719, 0.726247, 0.35639 , 0.331628,\n",
    " 0.336678, 0.409918, 0.333936, 0.364744, 0.365871, 0.386948, 0.335592,\n",
    " 0.433171, 0.405974, 0.370953, 0.342711, 0.34946 , 0.354049, 0.345798,\n",
    " 0.409167, 0.422822, 0.431098, 0.380923, 0.999865, 0.328017, 0.436479,\n",
    " 0.383351, 0.408608, 0.378319, 0.35766 , 0.357148, 0.447745, 0.377814,\n",
    " 0.403401, 0.381163, 0.309031, 0.34001 , 0.418265, 0.405636, 0.353707,\n",
    " 0.384461, 0.409158, 0.341729, 0.37811 , 0.36334 , 0.382874, 0.342945,\n",
    " 0.406179, 0.328214, 0.418591, 0.373767, 0.414642, 0.390589, 1.553005,\n",
    " 0.36636 , 0.38601 , 0.387245, 0.379337, 0.323819, 0.372452, 0.378096,\n",
    " 0.449504, 0.493522, 0.346804, 0.456164, 0.337032, 0.344737, 0.36811 ,\n",
    " 0.364335, 0.411038, 0.355447, 0.343541, 0.368234, 0.372744, 0.328724,\n",
    " 0.370818, 0.335024, 0.345697, 0.359749, 0.346284, 0.451509, 0.399984,\n",
    " 0.401827, 0.328459, 0.340872, 0.344964, 0.434994, 0.425496, 0.342075,\n",
    " 0.334118, 0.384962, 0.344306, 0.333449, 0.349411, 0.398999, 0.334388,\n",
    " 0.38535 , 0.35516 , 0.357367, 0.336457, 0.353116, 0.368034, 0.334302,\n",
    " 0.363616, 0.401013, 0.404806, 0.570224, 0.373265, 0.387134, 0.381473,\n",
    " 0.386238, 0.361729, 0.36757 , 0.36258 , 0.351058, 0.348397, 0.303029,\n",
    " 0.329093, 0.389602, 0.400014, 0.39492 , 0.32199 , 0.339261, 0.546789,\n",
    " 0.417079, 0.44158 , 0.392221, 0.342092, 0.386849, 0.354751, 0.332412,\n",
    " 0.35734 , 0.343799, 0.314097, 0.360053, 0.349872, 0.397325, 0.387486,\n",
    " 0.358562, 0.474963, 0.328008, 0.368968, 0.361688, 0.41013 , 0.31402 ,\n",
    " 0.36286 , 0.37216 , 0.397637, 0.353732, 0.317009, 0.345562, 0.422163,\n",
    " 0.414963, 0.428423, 0.391229, 0.379748, 0.440133, 0.368725, 0.434537,\n",
    " 0.363904, 0.349238, 0.430778, 0.358736, 0.394428, 0.398613, 0.396023,\n",
    " 0.383954, 0.359332, 0.360861, 0.406312, 0.427749, 0.389364, 0.463187,\n",
    " 0.451117, 0.396004, 0.414426, 0.362394, 0.348408, 0.550115, 0.351228,\n",
    " 0.382533, 0.400432, 0.459623, 0.162926, 0.355735, 0.447021, 0.356055,\n",
    " 0.397573, 0.365819, 0.402943, 0.333119, 0.324091, 0.358501, 0.338856,\n",
    " 0.340184, 0.375228, 0.398598, 0.367537, 0.323846, 0.400226, 0.416921,\n",
    " 0.364579, 0.416977, 0.33611 , 0.373105, 0.438842, 0.381575, 0.537092,\n",
    " 0.393436, 0.366145, 0.394156, 0.396069, 0.609297, 0.334615, 0.383501,\n",
    " 0.359216, 0.394509, 0.293119, 0.405207, 0.342539, 0.390912, 0.476695,\n",
    " 0.41978 , 0.377622, 0.370947, 0.326819, 0.333405, 0.343696, 0.342359,\n",
    " 0.404178, 0.425268, 0.402251, 0.397839, 0.329439, 0.346907, 0.388717,\n",
    " 0.36009 , 0.332785, 0.436224, 0.359073, 0.402372, 0.337193, 0.355489,\n",
    " 0.372543, 0.304874, 0.358558, 0.389601, 0.352411, 0.348189, 0.393774,\n",
    " 0.323663, 0.432197, 0.364874, 0.34804 , 0.389686, 0.389935, 0.359731,\n",
    " 0.458914, 0.326466, 0.372656, 0.406889, 0.401185, 0.36108 , 0.394925,\n",
    " 0.367963, 0.352859, 0.363547, 0.395707, 0.343955, 0.342901, 0.363041,\n",
    " 0.37017 , 0.368379, 0.417851, 0.310712, 0.394189, 0.361122, 0.349512,\n",
    " 0.453352, 0.623405, 0.323998, 0.357325, 0.465896, 0.375007, 0.735307,\n",
    " 0.605322, 0.326742, 0.432457, 0.426378, 0.435915, 0.720754, 0.348498,\n",
    " 0.396606, 0.349293, 0.378681, 0.46485 , 0.368582, 0.399799, 0.457163,\n",
    " 0.323962, 0.334734, 0.371112, 0.348894, 0.435195, 0.374729, 0.352945,\n",
    " 0.425604, 0.396221, 0.362775, 0.367653, 0.382813, 0.335422, 0.35947 ,\n",
    " 0.348202, 0.345453, 0.499921, 0.371636, 0.381015, 0.367283, 0.352896,\n",
    " 0.407755, 0.447115, 0.416769, 0.344713, 0.40774 , 0.345914, 0.348138,\n",
    " 0.354221, 0.315372, 0.393166, 0.358627, 0.336708, 0.390839]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 总样本数：327979008\n",
    "# 全局平均均值：0.017610\n",
    "# 全局平均标准差：0.385570\n",
    "\n",
    "mean = [-1.310834e-01,  1.005917e-02, -1.615213e-01,  3.709992e-02,  1.624055e-02,\n",
    " -1.072122e-01,  1.153811e-01, -1.764687e-01, -1.564773e-01,  8.010731e-02,\n",
    " -5.455930e-02,  1.026858e-02,  1.395730e-01, -1.677353e-01,  1.416284e-01,\n",
    "  9.624751e-02,  1.084609e-01, -2.143495e-01, -1.287082e-01, -8.222034e-03,\n",
    " -5.636413e-02,  8.554016e-02,  2.261810e-02,  6.544225e-02,  1.489254e-01,\n",
    " -4.932647e-02, -2.373628e-03,  1.595035e-01, -9.460699e-02,  3.337105e-02,\n",
    "  3.061204e-02,  3.003200e-02,  1.734831e-01,  3.377457e-02, -1.021726e-03,\n",
    " -6.289682e-02,  4.971703e-02, -3.472250e-03,  5.610131e-02, -7.359679e-02,\n",
    " -1.849378e-01,  2.845718e-02,  1.893580e-02, -1.241055e-02,  6.904855e-02,\n",
    "  9.557210e-02,  9.049045e-02, -2.354894e-02, -1.821717e-01,  2.173927e-01,\n",
    "  2.623105e-02, -7.295181e-02,  1.777242e-02,  1.112762e+00, -1.263934e-01,\n",
    " -1.298604e-01, -1.510958e-01,  4.459376e-02,  1.828927e-02, -9.131035e-02,\n",
    " -1.995497e-02, -6.222903e-04,  8.596381e-02,  4.136778e-02, -1.578171e-01,\n",
    " -1.337673e-01, -6.054174e-02,  7.156977e-03,  4.524891e-02,  1.216494e-01,\n",
    "  1.205348e-01,  1.033495e-01, -6.540378e-03,  1.981408e-01,  1.220096e-01,\n",
    "  1.036365e-01, -5.540284e-02, -4.624384e-02,  7.136753e-02,  3.484082e-02,\n",
    "  1.984291e-02, -4.273683e-02,  5.246440e-04, -2.307109e-02,  5.547218e-02,\n",
    " -3.933978e-02, -1.458146e-01, -1.129343e-02, -9.579260e-02,  9.284283e-02,\n",
    "  7.371892e-02, -7.637257e-02,  6.592041e-02, -8.843110e-02, -4.001666e-02,\n",
    " -3.148256e-02,  2.121865e-01,  4.097600e-02, -1.293690e-01, -9.589621e-02,\n",
    "  1.258695e-01, -7.101674e-02, -1.331008e-01,  1.493056e-02,  3.541788e+00,\n",
    "  1.420594e-02,  2.701039e-02,  1.175985e-01, -7.834096e-02,  6.380679e-02,\n",
    " -1.653035e-01,  1.550233e-01, -6.797676e-02, -5.019448e-02, -1.963801e-02,\n",
    "  6.645158e-02, -7.009321e-02, -8.109701e-02, -6.408926e-02, -1.683408e-02,\n",
    "  6.049670e-02, -4.398737e-02,  6.312481e-02, -7.364175e-02, -1.522977e-02,\n",
    "  3.527756e-02,  3.163086e-02,  1.245582e-01, -6.687518e-02,  2.580775e-01,\n",
    " -6.847859e-02,  2.051908e-01, -1.123238e-01, -3.409313e-02, -1.933814e-02,\n",
    " -8.682779e-02, -1.050035e-02,  1.283100e-02, -6.638120e-02,  7.655532e-02,\n",
    " -1.195735e-01,  9.798126e-02,  7.605009e-03, -2.112408e-03,  8.539424e-02,\n",
    "  5.429722e-02,  6.980925e-02,  1.459332e-01,  1.205048e-01, -4.054849e-02,\n",
    " -1.245425e-02, -1.228745e-01, -1.627585e-01,  8.031902e-02,  3.292367e-02,\n",
    " -3.371557e-02,  7.386365e-02,  1.811896e-02, -3.936293e-02, -1.017560e-01,\n",
    "  1.424287e-02, -5.966699e-02, -8.968184e-02,  1.662716e-01, -4.915838e-02,\n",
    " -2.737200e-04,  1.414123e-04,  1.011522e-01,  9.977543e-02,  1.224399e-02,\n",
    " -5.626816e-02, -1.061573e-01,  1.235399e-02, -2.361281e-02, -1.172489e-01,\n",
    "  3.690684e-02,  8.034723e-02, -5.032025e-02,  2.349837e-02, -3.136103e-02,\n",
    "  2.231061e-02, -2.128829e-02, -2.515885e-02, -3.194281e-02,  2.816122e-01,\n",
    "  4.188256e-01,  4.135317e-02,  6.339578e-02, -2.925155e-03,  6.940086e-02,\n",
    "  4.784072e-03, -6.444765e-02,  2.542174e-02,  9.924235e-02, -7.811626e-02,\n",
    "  4.030540e-02, -1.033520e-01,  1.386714e-01,  6.809492e-02,  1.093975e-01,\n",
    " -1.256160e-01,  3.689506e-02, -1.856487e-02,  1.850720e-02, -2.637269e-03,\n",
    "  4.571246e-02, -3.289911e-02, -1.077910e-01, -1.388599e-01,  1.328334e-01,\n",
    " -1.743474e-02, -1.308220e-01,  6.733084e-02,  6.298131e-03, -7.544571e-02,\n",
    "  4.740706e-02, -1.270548e-02, -4.204702e-02,  3.411375e-02, -5.634064e-02,\n",
    "  2.137969e-02,  1.725436e-03, -6.205269e-04,  4.907633e-02, -3.407640e-02,\n",
    " -6.360821e-02, -1.577485e-01,  5.634270e-02,  1.308212e-01,  1.170070e+00,\n",
    " -1.998444e-03, -7.140879e-02,  9.566184e-02,  1.889873e-02, -1.816112e+00,\n",
    "  3.908061e-02,  2.467009e-01, -1.227431e-01,  1.497491e-04,  1.018106e-01,\n",
    " -2.515418e-02, -8.447559e-03,  1.499944e-02,  1.022219e-01, -1.997879e-01,\n",
    " -6.360095e-02, -1.213443e-02,  1.518828e-02, -9.520721e-02, -7.753776e-02,\n",
    " -1.922333e-01,  1.406522e-01, -4.912835e-04,  2.104428e-01, -1.150434e-01,\n",
    "  1.147737e-01, -7.678745e-02,  1.235034e-01,  4.065556e-01,  5.981757e-02,\n",
    "  1.165455e-01,  5.487989e-02,  1.984056e-02,  8.687328e-02, -3.526523e-02,\n",
    "  6.872236e-02,  5.857139e-02, -4.834512e-03,  5.304520e-02, -1.590945e-01,\n",
    "  1.254610e-03,  8.046521e-02, -9.876981e-02, -1.112976e-01,  4.069212e-02,\n",
    "  7.972042e-03,  3.892454e-02,  3.154753e-02, -1.163380e-01, -1.605345e-01,\n",
    " -2.693421e-02,  2.297652e-03, -6.363157e-02, -8.444591e-02,  6.560753e-02,\n",
    "  3.419109e-02,  7.927979e-02, -2.809229e-02,  2.600385e-02,  4.144510e-03,\n",
    " -6.732618e-02,  1.544127e-02, -5.075310e-02, -3.481055e-02,  1.736548e-01,\n",
    " -8.164277e-03,  4.955614e-02, -2.437043e-02,  1.747116e-01,  9.048950e-03,\n",
    " -5.758368e-02, -2.327394e-02,  7.905414e-02, -4.244114e-02,  1.366992e-01,\n",
    "  7.400996e-02, -6.351946e-02,  2.505964e-01,  4.201991e-02, -3.152071e-02,\n",
    "  4.283984e-02, -1.246115e-01,  1.215930e-03, -2.306744e-02,  6.073258e-03,\n",
    " -4.608684e-02,  1.002165e-01,  9.539279e-02, -6.975203e-02,  3.450400e-03,\n",
    "  2.194004e-01, -1.432610e-01, -4.289287e-02, -2.807875e-02,  1.326494e-01,\n",
    "  7.334528e-03,  7.678882e-02, -1.392940e-01, -1.010023e-01,  8.594871e-03,\n",
    " -2.403476e-01, -2.865965e-02,  1.044892e-01, -3.868561e-02,  3.841664e-02,\n",
    "  9.703652e-01,  3.661903e-01, -5.823032e-02, -1.782770e-02,  1.497335e-02,\n",
    " -1.635204e-01, -1.064500e-01,  1.317102e-02, -1.928703e-02,  2.493870e-02,\n",
    "  1.109576e-01, -1.168163e-01,  1.212116e-03, -2.840918e-02,  1.174337e-01,\n",
    "  4.394750e-02,  1.290209e-01,  5.055910e-02, -5.226092e-02,  4.705502e-03,\n",
    " -9.981087e-02,  4.069665e-02, -1.311832e-02,  4.786288e-02,  2.846044e-01,\n",
    " -1.231434e-01, -2.413833e-01, -3.822717e-02,  7.206929e-02,  3.475883e-02,\n",
    "  8.923329e-02, -1.087692e-01, -8.315098e-02,  7.632284e-02,  1.032477e-01,\n",
    " -6.948436e-02, -8.174263e-02,  1.075035e-01,  1.116839e-01, -8.545190e-02,\n",
    " -2.812013e-02, -1.043946e-01,  5.158497e-02, -2.432621e-02,  3.674056e-02,\n",
    " -6.595597e-02, -1.876546e-01, -5.529918e-02, -1.807292e-04,  2.826878e-01,\n",
    "  6.332213e-02, -5.171585e-02, -6.061994e-01, -2.599412e-01, -2.190587e-01,\n",
    "  5.472286e-03,  9.263148e-01]\n",
    "\n",
    "std = [0.392849, 0.375289, 0.370822, 0.395598, 0.34522 , 0.405549, 0.361483,\n",
    " 0.366378, 0.31362 , 0.412469, 0.375657, 0.393863, 0.330323, 0.410793,\n",
    " 0.359335, 0.396973, 0.344869, 0.34835 , 0.349905, 0.392577, 0.381021,\n",
    " 0.328242, 0.395749, 0.380014, 0.38424 , 0.365401, 0.364199, 0.401468,\n",
    " 0.428667, 0.374665, 0.342948, 0.434147, 0.331311, 0.349698, 0.369843,\n",
    " 0.374011, 0.514408, 0.362731, 0.406831, 0.357253, 0.322817, 0.339967,\n",
    " 0.353882, 0.359335, 0.408583, 0.376324, 0.368155, 0.344663, 0.406418,\n",
    " 0.37508 , 0.420444, 0.334164, 0.374719, 0.726247, 0.35639 , 0.331628,\n",
    " 0.336678, 0.409918, 0.333936, 0.364744, 0.365871, 0.386948, 0.335592,\n",
    " 0.433171, 0.405974, 0.370953, 0.342711, 0.34946 , 0.354049, 0.345798,\n",
    " 0.409167, 0.422822, 0.431098, 0.380923, 0.999865, 0.328017, 0.436479,\n",
    " 0.383351, 0.408608, 0.378319, 0.35766 , 0.357148, 0.447745, 0.377814,\n",
    " 0.403401, 0.381163, 0.309031, 0.34001 , 0.418265, 0.405636, 0.353707,\n",
    " 0.384461, 0.409158, 0.341729, 0.37811 , 0.36334 , 0.382874, 0.342945,\n",
    " 0.406179, 0.328214, 0.418591, 0.373767, 0.414642, 0.390589, 1.553005,\n",
    " 0.36636 , 0.38601 , 0.387245, 0.379337, 0.323819, 0.372452, 0.378096,\n",
    " 0.449504, 0.493522, 0.346804, 0.456164, 0.337032, 0.344737, 0.36811 ,\n",
    " 0.364335, 0.411038, 0.355447, 0.343541, 0.368234, 0.372744, 0.328724,\n",
    " 0.370818, 0.335024, 0.345697, 0.359749, 0.346284, 0.451509, 0.399984,\n",
    " 0.401827, 0.328459, 0.340872, 0.344964, 0.434994, 0.425496, 0.342075,\n",
    " 0.334118, 0.384962, 0.344306, 0.333449, 0.349411, 0.398999, 0.334388,\n",
    " 0.38535 , 0.35516 , 0.357367, 0.336457, 0.353116, 0.368034, 0.334302,\n",
    " 0.363616, 0.401013, 0.404806, 0.570224, 0.373265, 0.387134, 0.381473,\n",
    " 0.386238, 0.361729, 0.36757 , 0.36258 , 0.351058, 0.348397, 0.303029,\n",
    " 0.329093, 0.389602, 0.400014, 0.39492 , 0.32199 , 0.339261, 0.546789,\n",
    " 0.417079, 0.44158 , 0.392221, 0.342092, 0.386849, 0.354751, 0.332412,\n",
    " 0.35734 , 0.343799, 0.314097, 0.360053, 0.349872, 0.397325, 0.387486,\n",
    " 0.358562, 0.474963, 0.328008, 0.368968, 0.361688, 0.41013 , 0.31402 ,\n",
    " 0.36286 , 0.37216 , 0.397637, 0.353732, 0.317009, 0.345562, 0.422163,\n",
    " 0.414963, 0.428423, 0.391229, 0.379748, 0.440133, 0.368725, 0.434537,\n",
    " 0.363904, 0.349238, 0.430778, 0.358736, 0.394428, 0.398613, 0.396023,\n",
    " 0.383954, 0.359332, 0.360861, 0.406312, 0.427749, 0.389364, 0.463187,\n",
    " 0.451117, 0.396004, 0.414426, 0.362394, 0.348408, 0.550115, 0.351228,\n",
    " 0.382533, 0.400432, 0.459623, 0.162926, 0.355735, 0.447021, 0.356055,\n",
    " 0.397573, 0.365819, 0.402943, 0.333119, 0.324091, 0.358501, 0.338856,\n",
    " 0.340184, 0.375228, 0.398598, 0.367537, 0.323846, 0.400226, 0.416921,\n",
    " 0.364579, 0.416977, 0.33611 , 0.373105, 0.438842, 0.381575, 0.537092,\n",
    " 0.393436, 0.366145, 0.394156, 0.396069, 0.609297, 0.334615, 0.383501,\n",
    " 0.359216, 0.394509, 0.293119, 0.405207, 0.342539, 0.390912, 0.476695,\n",
    " 0.41978 , 0.377622, 0.370947, 0.326819, 0.333405, 0.343696, 0.342359,\n",
    " 0.404178, 0.425268, 0.402251, 0.397839, 0.329439, 0.346907, 0.388717,\n",
    " 0.36009 , 0.332785, 0.436224, 0.359073, 0.402372, 0.337193, 0.355489,\n",
    " 0.372543, 0.304874, 0.358558, 0.389601, 0.352411, 0.348189, 0.393774,\n",
    " 0.323663, 0.432197, 0.364874, 0.34804 , 0.389686, 0.389935, 0.359731,\n",
    " 0.458914, 0.326466, 0.372656, 0.406889, 0.401185, 0.36108 , 0.394925,\n",
    " 0.367963, 0.352859, 0.363547, 0.395707, 0.343955, 0.342901, 0.363041,\n",
    " 0.37017 , 0.368379, 0.417851, 0.310712, 0.394189, 0.361122, 0.349512,\n",
    " 0.453352, 0.623405, 0.323998, 0.357325, 0.465896, 0.375007, 0.735307,\n",
    " 0.605322, 0.326742, 0.432457, 0.426378, 0.435915, 0.720754, 0.348498,\n",
    " 0.396606, 0.349293, 0.378681, 0.46485 , 0.368582, 0.399799, 0.457163,\n",
    " 0.323962, 0.334734, 0.371112, 0.348894, 0.435195, 0.374729, 0.352945,\n",
    " 0.425604, 0.396221, 0.362775, 0.367653, 0.382813, 0.335422, 0.35947 ,\n",
    " 0.348202, 0.345453, 0.499921, 0.371636, 0.381015, 0.367283, 0.352896,\n",
    " 0.407755, 0.447115, 0.416769, 0.344713, 0.40774 , 0.345914, 0.348138,\n",
    " 0.354221, 0.315372, 0.393166, 0.358627, 0.336708, 0.390839, 0.365   ,\n",
    " 0.284646, 0.342384, 0.333129, 0.331953, 0.415125, 0.31927 , 0.327767]\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "dinov3_sp_mean = torch.tensor(mean).reshape(1, 1, 392)\n",
    "dinov3_sp_std = torch.tensor(std).reshape(1, 1, 392)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dinov3_sp_mean.shape, dinov3_sp_std.shape\n",
    "dinov3_sp_stats = {\n",
    "    \"dinov3_sp_mean\": dinov3_sp_mean[:,:,:392],\n",
    "    \"dinov3_sp_std\": dinov3_sp_std[:,:,:392],\n",
    "\n",
    "}\n",
    "\n",
    "torch.save(dinov3_sp_stats, \"dinov3_sp_stats.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(\"dinov3_sp_stats.pt\")[\"dinov3_sp_std\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[18.7307],\n",
       "        [19.9318],\n",
       "        [19.8622],\n",
       "        [19.2694],\n",
       "        [19.3598],\n",
       "        [20.3030],\n",
       "        [19.2964],\n",
       "        [21.0907],\n",
       "        [18.5853],\n",
       "        [20.5296],\n",
       "        [19.1187],\n",
       "        [20.0649],\n",
       "        [20.3333],\n",
       "        [19.8554],\n",
       "        [21.0110],\n",
       "        [19.5088],\n",
       "        [19.1574],\n",
       "        [20.1965],\n",
       "        [20.1290],\n",
       "        [19.8439],\n",
       "        [18.9291],\n",
       "        [21.0411],\n",
       "        [18.2661],\n",
       "        [19.7600],\n",
       "        [19.8232],\n",
       "        [20.6046],\n",
       "        [19.7292],\n",
       "        [19.6593],\n",
       "        [20.7582],\n",
       "        [18.7646],\n",
       "        [19.2895],\n",
       "        [19.2632],\n",
       "        [20.4167],\n",
       "        [19.0856],\n",
       "        [20.0163],\n",
       "        [19.2354],\n",
       "        [18.5673],\n",
       "        [18.6294],\n",
       "        [18.7435],\n",
       "        [19.5231],\n",
       "        [19.1054],\n",
       "        [19.2286],\n",
       "        [18.9322],\n",
       "        [19.1706],\n",
       "        [20.2459],\n",
       "        [18.8396],\n",
       "        [20.7503],\n",
       "        [20.3546],\n",
       "        [20.6769],\n",
       "        [19.1410],\n",
       "        [19.7409],\n",
       "        [19.3085],\n",
       "        [20.6736],\n",
       "        [18.3143],\n",
       "        [19.6429],\n",
       "        [19.5610],\n",
       "        [19.8740],\n",
       "        [18.9069],\n",
       "        [19.0231],\n",
       "        [18.6301],\n",
       "        [19.0216],\n",
       "        [20.0375],\n",
       "        [18.9875],\n",
       "        [19.9287],\n",
       "        [19.6429],\n",
       "        [18.8329],\n",
       "        [20.1847],\n",
       "        [19.4167],\n",
       "        [19.8214],\n",
       "        [18.8153],\n",
       "        [20.2739],\n",
       "        [19.2363],\n",
       "        [19.2013],\n",
       "        [19.4045],\n",
       "        [19.6670],\n",
       "        [20.0633],\n",
       "        [18.7985],\n",
       "        [20.9444],\n",
       "        [19.0127],\n",
       "        [19.5857],\n",
       "        [20.9961],\n",
       "        [19.0849],\n",
       "        [19.7540],\n",
       "        [18.8193],\n",
       "        [19.5025],\n",
       "        [19.9219],\n",
       "        [19.6245],\n",
       "        [18.8774],\n",
       "        [20.0342],\n",
       "        [19.3466],\n",
       "        [19.2087],\n",
       "        [20.1744],\n",
       "        [19.8991],\n",
       "        [18.9130],\n",
       "        [19.5336],\n",
       "        [20.6950],\n",
       "        [20.2152],\n",
       "        [19.3413],\n",
       "        [20.2060],\n",
       "        [19.6427],\n",
       "        [19.6602],\n",
       "        [19.6904],\n",
       "        [17.8799],\n",
       "        [20.4513],\n",
       "        [19.5326],\n",
       "        [20.5958],\n",
       "        [19.1593],\n",
       "        [20.2456],\n",
       "        [20.6699],\n",
       "        [19.3174],\n",
       "        [20.0280],\n",
       "        [20.3418],\n",
       "        [18.9142],\n",
       "        [18.7422],\n",
       "        [20.3505],\n",
       "        [19.4827],\n",
       "        [20.7044],\n",
       "        [19.2138],\n",
       "        [19.6026],\n",
       "        [19.4172],\n",
       "        [18.9426],\n",
       "        [19.2803],\n",
       "        [19.2713],\n",
       "        [20.3774],\n",
       "        [18.5991],\n",
       "        [19.9182],\n",
       "        [19.3457],\n",
       "        [18.9697],\n",
       "        [19.6511],\n",
       "        [18.7573],\n",
       "        [19.0971],\n",
       "        [18.3513],\n",
       "        [19.4046],\n",
       "        [19.4017],\n",
       "        [19.9411],\n",
       "        [17.7343],\n",
       "        [19.7051],\n",
       "        [19.4988],\n",
       "        [19.3561],\n",
       "        [18.3604],\n",
       "        [19.0007],\n",
       "        [19.4463],\n",
       "        [18.9132],\n",
       "        [19.7720],\n",
       "        [20.3305],\n",
       "        [18.7813],\n",
       "        [19.7396],\n",
       "        [18.5375],\n",
       "        [21.0291],\n",
       "        [20.3985],\n",
       "        [19.7081],\n",
       "        [20.0042],\n",
       "        [19.0128],\n",
       "        [19.0030],\n",
       "        [19.4012],\n",
       "        [19.7495],\n",
       "        [19.5846],\n",
       "        [19.4809],\n",
       "        [20.1786],\n",
       "        [18.9039],\n",
       "        [19.8192],\n",
       "        [19.7451],\n",
       "        [19.7350],\n",
       "        [19.4945],\n",
       "        [18.7037],\n",
       "        [20.0050],\n",
       "        [20.0338],\n",
       "        [19.4591],\n",
       "        [19.5558],\n",
       "        [20.4425],\n",
       "        [21.1239],\n",
       "        [19.0935],\n",
       "        [19.8697],\n",
       "        [19.6715],\n",
       "        [19.4512],\n",
       "        [19.4812],\n",
       "        [18.9607],\n",
       "        [19.1512],\n",
       "        [18.9170],\n",
       "        [18.5487],\n",
       "        [19.3669],\n",
       "        [20.4699],\n",
       "        [19.7550],\n",
       "        [20.3530],\n",
       "        [18.5405],\n",
       "        [20.2040],\n",
       "        [19.4965],\n",
       "        [19.0918],\n",
       "        [20.1177],\n",
       "        [19.5907],\n",
       "        [19.2003],\n",
       "        [20.0993],\n",
       "        [19.9405],\n",
       "        [19.3769],\n",
       "        [20.2669],\n",
       "        [19.7850],\n",
       "        [18.9770],\n",
       "        [19.2068],\n",
       "        [18.9702],\n",
       "        [19.7079],\n",
       "        [19.6755],\n",
       "        [19.5926],\n",
       "        [18.3048],\n",
       "        [20.4867],\n",
       "        [19.9051],\n",
       "        [20.1951],\n",
       "        [19.2139],\n",
       "        [19.6372],\n",
       "        [19.8072],\n",
       "        [19.4133],\n",
       "        [19.9415],\n",
       "        [19.9165],\n",
       "        [18.7316],\n",
       "        [18.8317],\n",
       "        [20.6404],\n",
       "        [19.5154],\n",
       "        [19.2134],\n",
       "        [20.1251],\n",
       "        [19.1987],\n",
       "        [19.7633],\n",
       "        [18.6299],\n",
       "        [19.1805],\n",
       "        [19.7459],\n",
       "        [18.8382],\n",
       "        [19.4181],\n",
       "        [19.3941],\n",
       "        [18.7293],\n",
       "        [20.8518],\n",
       "        [18.8390],\n",
       "        [19.3876],\n",
       "        [20.0894],\n",
       "        [19.9605],\n",
       "        [19.6220],\n",
       "        [19.3656],\n",
       "        [18.8697],\n",
       "        [20.5252],\n",
       "        [19.6097],\n",
       "        [18.4781],\n",
       "        [20.5218],\n",
       "        [19.8649],\n",
       "        [20.2826],\n",
       "        [17.9214],\n",
       "        [19.2533],\n",
       "        [19.7497],\n",
       "        [19.9166],\n",
       "        [20.7741],\n",
       "        [19.6972],\n",
       "        [20.3370],\n",
       "        [19.0664],\n",
       "        [19.3542],\n",
       "        [19.9605],\n",
       "        [20.9962],\n",
       "        [19.4973],\n",
       "        [19.8475],\n",
       "        [18.6649],\n",
       "        [20.0871]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "eps = torch.randn((16*16, 384))\n",
    "norm = torch.norm(eps, p=2, dim=-1, keepdim=True)\n",
    "norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
